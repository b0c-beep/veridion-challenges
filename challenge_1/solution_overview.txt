Project Overview:

	The objective of this project was to extract and validate addresses from various websites, consolidating results into an Excel report. I used libraries like BeautifulSoup for scraping, pyap for address extraction, usaddress for address formatting, and geopy for address validation. By the end, I had a structured pipeline to gather, parse, validate, and report address data.

Key Steps and Challenges:

Web Scraping with BeautifulSoup
Challenge: Some websites blocked scraping attempts.
Solution: I tackled this by setting up custom headers, simulating a browser request to bypass basic bot detection. Also refined error handling to manage blocked URLs more gracefully, ensuring the script could continue with reachable sites. Not all sites are reachable even with these contingencies in place.

Content Extraction from Reachable Sites
Challenge: Extracted content did not pull 100% of website content.
Solution: Parsed every internal link found to make sure extraction is as close to 100% as possible.

Address Extraction with pyap
Challenge: Inconsistent Detection of Addresses: pyap only partially detected addresses, especially for complex structures or non-standard formats.
Solution: I handled this by setting up fallback strategies and ensuring the script collected as much detail as possible by cleaning the extracted website content for easier interpretation. However, pyap results required further cleaning and verification, not being entirely reliable.

Address Formatting and parsing with usaddress
Challenge: Handling Parsing Variability: usaddress returned parsed addresses as tuples, the library didn't format just valid addresses, some still contain gibberish.
Solution: A function was designed to map usaddress output to specific components, converting parsed tuples to dictionaries. This function standardized the format but required careful error handling to prevent empty or partial address entries, some results still being incorrect.

Address Validation with geopy
Challenge: Handling Ambiguous and Incorrect Validation Results: While geopy was instrumental for location validation, it occasionally validated addresses containing incomplete or nonsensical data, making the validation unreliable in some cases. Edge cases also include international addresses, the script is configured for US addresses.
Solution: To address this, I implemented filtering criteria to clean the scraped content even more but some invalid addresses still pass validation.

Reporting in Excel
Challenge: Accurate Reporting and Summarization: Formatting and analyzing data in Excel required specific formulas, especially for calculating percentages of validated addresses.
Solution: I created customized Excel functions to summarize the findings effectively, including the percentage of validated addresses, handling conditions like “Not validated,” and using conditional formatting for clarity. Also added real-time stats for the terminal.

Contents of project: 
	Scripts:
		-challenge1.py - the actual script which scrapes and validates the content and saves it to the excel file
		-save_excel.py - script to save results and create backups for important checkpoints
		-bf4_testground.py - testing ground script, messing with new functions and trying to implement new ideas for specific websites, some additions made it into the final script, some did not
	Excel files: results.xlsx contains the results generated by the last call of challenge1.py, all other excel files are backups made at certain points during development.
	The snappy.parquet file contains the list of websites to scrape and validate.

Libraries and Tools
	Python - Core programming language for scripting and automation.
	BeautifulSoup - For web scraping and parsing HTML content from websites.
	pyap - Used for address extraction, focusing on pulling address patterns from text.
	usaddress - For parsing and formatting addresses into structured components.
	geopy - Used for validating address accuracy via geolocation.
	openpyxl / pandas - For handling Excel files, saving, and formatting data.

Conclusion: While the script isn't 100% successful, I am content with the work I was able to do during this week of development, the solution can still be greatly improved.
Suggestions: Adding functionality for international addresses, refining the content formatting even more, improved methods of validation and extraction, exploring new libraries for better results.